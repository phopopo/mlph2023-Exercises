{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Phase classification in $\\phi^4$ theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"data05/lattice_train.npy\")\n",
    "test = np.load(\"data05/lattice_test.npy\")\n",
    "kappa = np.arange(0.24, 0.30, 0.0025)\n",
    "\n",
    "L = 16\n",
    "lattice_shape = (L,L)\n",
    "\n",
    "train, test = torch.tensor(train), torch.tensor(test)\n",
    "symmetric_phase_data, broken_phase_data = train[0,...], train[1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize lattice sides for different values of kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Plot means and stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Uncertainties in Amplitude Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:01<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mtutorial-2-data\u001b[0m/  \u001b[01;32mtutorial-2-data.zip\u001b[0m*\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download data to folder data04\n",
    "# this might take some time (50MB)\n",
    "# you can also do this manually (download + unpack zip)\n",
    "import os, sys\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "DESTINATION = \"data04\"\n",
    "url = \"https://www.thphys.uni-heidelberg.de/~plehn/pics/\"\n",
    "filename = \"tutorial-2-data.zip\"\n",
    "url = url + filename\n",
    "\n",
    "os.makedirs(DESTINATION, exist_ok=True)\n",
    "os.chdir(DESTINATION)\n",
    "wget.download(url, filename)\n",
    "with ZipFile(filename, \"r\") as zip_ref:\n",
    "    for file in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist())):\n",
    "        zip_ref.extract(member=file)\n",
    "%ls\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (30000, 5, 4)\n",
      "train amp  shape: (30000,)\n",
      "test  data shape: (30000, 5, 4)\n",
      "test  amp  shape: (30000,)\n",
      "val   data shape: (30000, 5, 4)\n",
      "val   amp  shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "trn_dat = np.load(\"data04/tutorial-2-data/trn_dat.npy\")\n",
    "trn_amp = np.load(\"data04/tutorial-2-data/trn_amp.npy\")\n",
    "\n",
    "val_dat = np.load(\"data04/tutorial-2-data/val_dat.npy\")\n",
    "val_amp = np.load(\"data04/tutorial-2-data/val_amp.npy\")\n",
    "\n",
    "tst_dat = np.load(\"data04/tutorial-2-data/tst_dat.npy\")\n",
    "tst_amp = np.load(\"data04/tutorial-2-data/tst_amp.npy\")\n",
    "\n",
    "print(f\"train data shape: {trn_dat.shape}\")\n",
    "print(f\"train amp  shape: {trn_amp.shape}\")\n",
    "print(f\"test  data shape: {tst_dat.shape}\")\n",
    "print(f\"test  amp  shape: {tst_amp.shape}\")\n",
    "print(f\"val   data shape: {val_dat.shape}\")\n",
    "print(f\"val   amp  shape: {val_amp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recycle code from last sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train ensemble of deterministic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate ensemble -> List of predictions for log A\n",
    "# result: array preds of shape (10, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainties at event level\n",
    "\n",
    "preds_means = np.mean(np.log(preds), axis=0)\n",
    "preds_stds = np.std(np.log(preds), axis=0)\n",
    "\n",
    "plt.hist(np.log(tst_amp), range=(-20, -8), bins=50, alpha=.5)\n",
    "plt.hist(preds_means, range=(-20, -8), bins=50, alpha=.5)\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin height\")\n",
    "plt.show()\n",
    "\n",
    "x = range(len(tst_amp))\n",
    "plt.fill_between(x, preds_stds, -preds_stds, color=\"y\", label=r\"predicted $\\sigma_{\\log A}$\")\n",
    "plt.plot(x, preds_means - np.log(tst_amp), \",\", alpha=.5, ms=.1, label=r\"$\\log A_\\mathrm{pred}-\\log A_\\mathrm{true}$\")\n",
    "plt.xlabel(\"# event\")\n",
    "plt.ylim(-1,1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainties at histogram level\n",
    "\n",
    "hist_test, bins = np.histogram(np.log(tst_amp), range=(-20,-8), bins=50)\n",
    "hist_data = np.array([np.histogram(np.log(preds[i,:]), bins=bins)[0] for i in range(n_models)])\n",
    "hist_means = np.mean(hist_data, axis=0)\n",
    "hist_stds = np.std(hist_data, axis=0)\n",
    "\n",
    "# means\n",
    "dup_last = lambda a: np.append(a, a[-1])\n",
    "plt.step(bins, dup_last(hist_test), label=\"test\")\n",
    "plt.step(bins, dup_last(hist_means), label=\"ensemble\")\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin height\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# standard deviations\n",
    "plt.step(bins, dup_last(np.sqrt(hist_means)), label=\"statistical uncertainty\")\n",
    "plt.step(bins, dup_last(hist_stds), label=\"ensemble uncertainty\")\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin standard deviation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# means + standard deviations\n",
    "hists = [hist_test, hist_means]\n",
    "hist_errors = [np.sqrt(hist_test), hist_stds]\n",
    "integrals = [np.sum((bins[1:] - bins[:-1])*y) for y in hists]\n",
    "scales = [1 / integral if integral != 0. else 1. for integral in integrals]\n",
    "\n",
    "label = [\"Test\", \"ensemble\"]\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, gridspec_kw={\"height_ratios\": [3, 1], \"hspace\": 0.})\n",
    "for y, y_err, scale, label in zip(hists, hist_errors, scales, label):\n",
    "    axs[0].step(bins, dup_last(y) * scale, label=label, where=\"post\")\n",
    "    axs[0].fill_between(bins, dup_last(y + y_err) * scale, \n",
    "                     dup_last(y - y_err) * scale, alpha=.5, step=\"post\")\n",
    "    \n",
    "    ratio = (y * scale) / (hists[0] * scales[0])\n",
    "    ratio_err = np.sqrt( (y_err/y)**2 + (hist_errors[0] / hists[0])**2)\n",
    "    ratio[np.isnan(ratio)] = 1.\n",
    "    ratio_err[np.isnan(ratio)] = 0.\n",
    "    \n",
    "    axs[1].step(bins, dup_last(ratio), where=\"post\")\n",
    "    axs[1].fill_between(bins, dup_last(ratio + ratio_err), dup_last(ratio - ratio_err), step=\"post\", alpha=.5)\n",
    "axs[1].set_xlabel(r\"$\\log A$\")\n",
    "axs[0].set_ylabel(\"Normalized\")\n",
    "axs[1].set_ylabel(r\"$\\frac{\\mathrm{Ensemble}}{\\mathrm{Test}}$\")\n",
    "axs[1].set_ylim(.5, 1.5)\n",
    "axs[0].legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in expression for KL divergence\n",
    "import math\n",
    "\n",
    "class VBLinear(nn.Module):\n",
    "    # VB = variational bayes\n",
    "    def __init__(self, in_features, out_features, prior_prec=1.0, _map=False, std_init=-5):\n",
    "        super(VBLinear, self).__init__()\n",
    "        self.n_in = in_features\n",
    "        self.n_out = out_features\n",
    "        self.map = _map\n",
    "        self.prior_prec = prior_prec # = 1/sigma_prior**2\n",
    "        self.random = None\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.mu_w = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.logsig2_w = nn.Parameter(torch.Tensor(out_features, in_features)) # log(sigma_network**2)\n",
    "        self.std_init = std_init\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.mu_w.size(1))\n",
    "        self.mu_w.data.normal_(0, stdv)\n",
    "        self.logsig2_w.data.zero_().normal_(self.std_init, 0.001)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def KL(self):\n",
    "        logsig2_w = self.logsig2_w.clamp(-11, 11)\n",
    "        kl = None # KL divergence of variational bayes linear layer \n",
    "        return kl\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # local reparameterization trick is more efficient and leads to\n",
    "            # an estimate of the gradient with smaller variance.\n",
    "            # https://arxiv.org/pdf/1506.02557.pdf\n",
    "            mu_out = nn.functional.linear(input, self.mu_w, self.bias)\n",
    "            logsig2_w = self.logsig2_w.clamp(-11, 11)\n",
    "            s2_w = logsig2_w.exp()\n",
    "            var_out = nn.functional.linear(input.pow(2), s2_w) + 1e-8\n",
    "            return mu_out + var_out.sqrt() * torch.randn_like(mu_out)\n",
    "\n",
    "        else:\n",
    "            if self.map: # just return the mean, no sampling\n",
    "                return nn.functional.linear(input, self.mu_w, self.bias)\n",
    "\n",
    "            logsig2_w = self.logsig2_w.clamp(-11, 11)\n",
    "            if self.random is None:\n",
    "                self.random = torch.randn_like(self.logsig2_w)\n",
    "            s2_w = logsig2_w.exp()\n",
    "            weight = self.mu_w + s2_w.sqrt() * self.random\n",
    "            return nn.functional.linear(input, weight, self.bias) + 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create BNN (like deterministic network, but with bayesian layers)\n",
    "\n",
    "class bayesian_amp_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, training_size, ipt_dim=20, opt_dim=1, hdn_dim=32, n_layers=2):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def KL(self):\n",
    "        kl = 0\n",
    "        for layer in self.bayesian_layers:\n",
    "            kl += layer.KL()\n",
    "        return kl\n",
    "    \n",
    "    def reset_BNN(self):\n",
    "        for layer in self.bayesian_layers:\n",
    "            layer.random = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss_bayesian(model, x, y):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y)\n",
    "    kl = model.KL() / trn_dat.shape[0] # correct normalization!\n",
    "    loss += kl\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train BNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainties at event level\n",
    "\n",
    "preds_means = np.mean(np.log(preds), axis=0)\n",
    "preds_stds = np.std(np.log(preds), axis=0)\n",
    "\n",
    "plt.hist(np.log(tst_amp), range=(-20, -8), bins=50, alpha=.5)\n",
    "plt.hist(preds_means, range=(-20, -8), bins=50, alpha=.5)\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin height\")\n",
    "plt.show()\n",
    "\n",
    "x = range(len(tst_amp))\n",
    "plt.fill_between(x, preds_stds, -preds_stds, color=\"y\", label=r\"predicted $\\sigma_{\\log A}$\")\n",
    "plt.plot(x, preds_means - np.log(tst_amp), \",\", alpha=.5, ms=.1, label=r\"$\\log A_\\mathrm{pred}-\\log A_\\mathrm{true}$\")\n",
    "plt.xlabel(\"# event\")\n",
    "plt.ylim(-1,1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainties at histogram level\n",
    "\n",
    "hist_test, bins = np.histogram(np.log(tst_amp), range=(-20,-8), bins=50)\n",
    "hist_data = np.array([np.histogram(np.log(preds[i,:]), bins=bins)[0] for i in range(n_models)])\n",
    "hist_means = np.mean(hist_data, axis=0)\n",
    "hist_stds = np.std(hist_data, axis=0)\n",
    "\n",
    "# means\n",
    "dup_last = lambda a: np.append(a, a[-1])\n",
    "plt.step(bins, dup_last(hist_test), label=\"test\")\n",
    "plt.step(bins, dup_last(hist_means), label=\"ensemble\")\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin height\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# standard deviations\n",
    "plt.step(bins, dup_last(np.sqrt(hist_means)), label=\"statistical uncertainty\")\n",
    "plt.step(bins, dup_last(hist_stds), label=\"ensemble uncertainty\")\n",
    "plt.xlabel(r\"$\\log A$\")\n",
    "plt.ylabel(\"bin standard deviation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# means + standard deviations\n",
    "hists = [hist_test, hist_means]\n",
    "hist_errors = [np.sqrt(hist_test), hist_stds]\n",
    "integrals = [np.sum((bins[1:] - bins[:-1])*y) for y in hists]\n",
    "scales = [1 / integral if integral != 0. else 1. for integral in integrals]\n",
    "\n",
    "label = [\"Test\", \"ensemble\"]\n",
    "fig, axs = plt.subplots(2, 1, sharex=True, gridspec_kw={\"height_ratios\": [3, 1], \"hspace\": 0.})\n",
    "for y, y_err, scale, label in zip(hists, hist_errors, scales, label):\n",
    "    axs[0].step(bins, dup_last(y) * scale, label=label, where=\"post\")\n",
    "    axs[0].fill_between(bins, dup_last(y + y_err) * scale, \n",
    "                     dup_last(y - y_err) * scale, alpha=.5, step=\"post\")\n",
    "    \n",
    "    ratio = (y * scale) / (hists[0] * scales[0])\n",
    "    ratio_err = np.sqrt( (y_err/y)**2 + (hist_errors[0] / hists[0])**2)\n",
    "    ratio[np.isnan(ratio)] = 1.\n",
    "    ratio_err[np.isnan(ratio)] = 0.\n",
    "    \n",
    "    axs[1].step(bins, dup_last(ratio), where=\"post\")\n",
    "    axs[1].fill_between(bins, dup_last(ratio + ratio_err), dup_last(ratio - ratio_err), step=\"post\", alpha=.5)\n",
    "axs[1].set_xlabel(r\"$\\log A$\")\n",
    "axs[0].set_ylabel(\"Normalized\")\n",
    "axs[1].set_ylabel(r\"$\\frac{\\mathrm{Ensemble}}{\\mathrm{Test}}$\")\n",
    "axs[1].set_ylim(.5, 1.5)\n",
    "axs[0].legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
