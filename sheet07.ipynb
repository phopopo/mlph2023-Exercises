{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Log-sum-exp and softmax\n",
    "\n",
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, lamb=1):\n",
    "    # TODO: implement the logsumexp\n",
    "\n",
    "# TODO: set up a grid of points in [-1, 1] x [-1, 1]\n",
    "\n",
    "\n",
    "# TODO: I recommend you set up a function to set up an Axes object with the correct x, y labels, \n",
    "#       equal aspect and maybe x and y ticks.\n",
    "\n",
    "def set_up_axes(ax):\n",
    "    \n",
    "# TODO: calculate and plot the functions as specified in the task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis, lamb=1):\n",
    "    # TODO: implement the softmax function. Axis should specify along which axis the sums should be computed.\n",
    "\n",
    "        \n",
    "# TODO: compute the argmax of each gridpoint in one-hot form\n",
    "onehot_argmax = to_onehot(np.argmax(xy, axis=-1))\n",
    "\n",
    "# TODO: make the plots as specified on the sheet (nicest is in a grid which you can get using plt.subplots)\n",
    "\n",
    "# plot the softmax\n",
    "fig, axs = plt.subplots(2, 4, figsize=(17, 7))\n",
    "        \n",
    "# plot the onehot argmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Top tagging with Point Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/heidelberg-hepml/ml-tutorials for solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) CNNs for Galaxy Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "label_names = [\n",
    "    'Disk, Face-on, No Spiral',\n",
    "    'Smooth, Completely round',\n",
    "    'Smooth, in-between round',\n",
    "    'Smooth, Cigar shaped',\n",
    "    'Disk, Edge-on, Rounded Bulge',\n",
    "    'Disk, Edge-on, Boxy Bulge',\n",
    "    'Disk, Edge-on, No Bulge',\n",
    "    'Disk, Face-on, Tight Spiral',\n",
    "    'Disk, Face-on, Medium Spiral',\n",
    "    'Disk, Face-on, Loose Spiral'\n",
    "]\n",
    "n_classes = len(label_names)\n",
    "\n",
    "# To get the images and labels from file\n",
    "with h5py.File('data07/Galaxy10.h5', 'r') as F:\n",
    "    images = np.array(F['images'])\n",
    "    labels = np.array(F['ans'])\n",
    "images = images.astype(np.float32)\n",
    "\n",
    "# comply to (batch, channel, height, width) convention of pytorch\n",
    "images = np.moveaxis(images, -1, 1)  \n",
    "# convert to torch\n",
    "images = torch.from_numpy(images)\n",
    "labels = torch.from_numpy(labels)\n",
    "\n",
    "print(f'{images.shape=}, {labels.shape=}')\n",
    "\n",
    "print(labels.shape, images.shape)\n",
    "# TODO: print the number of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# TODO: plot some samples of each class\n",
    "itemindex = torch.where(labels == 3)\n",
    "itemindex[:3]\n",
    "\n",
    "samples_per_class = 3\n",
    "fig, axss = plt.subplots(samples_per_class, n_classes, figsize=(n_classes * 2, samples_per_class * 2))\n",
    "for label, (label_name, axs) in enumerate(zip(label_names, axss.T)):\n",
    "    idx = torch.where(labels==label)[0][:samples_per_class] # take samples_per_class first occurences\n",
    "    for i, (ind, ax) in enumerate(zip(idx, axs)):\n",
    "        ax.imshow(images[ind].long().moveaxis(0, -1))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == 0:\n",
    "            ax.set_title(label_name.replace(',',',\\n'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "# TODO: Compute the mean and standard deviation per channel over the dataset\n",
    "\n",
    "# stds = images.moveaxis(1, 0).reshape(3, -1).std(axis=1)\n",
    "# means = images.moveaxis(1, 0).reshape(3, -1).mean(axis=1)\n",
    "stds, means = torch.tensor([37.5412, 31.3756, 26.3283]), torch.tensor([27.7014, 23.8241, 18.1425])\n",
    "print(stds, means)\n",
    "\n",
    "# TODO: Normalize the images\n",
    "normalize = Normalize(means, stds)\n",
    "images_normalized = normalize(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "n_samples = len(labels)\n",
    "\n",
    "# TODO: split the data in training and validation sets, stratifying by the labels\n",
    "train_idx, val_idx = train_test_split(np.arange(n_samples), test_size=0.1, stratify=labels)\n",
    "\n",
    "# TODO: create pytorch datasets for training and validation\n",
    "train_dataset = TensorDataset(images_normalized[train_idx].float(), labels[train_idx].long())\n",
    "val_dataset = TensorDataset(images_normalized[val_idx].float(), labels[val_idx].long())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#TODO: implement a small CNN as specified on the sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create DataLoaders for train and val, use a batch size of 16\n",
    "\n",
    "# TODO: instantiate the model, optimizer and criterion\n",
    "\n",
    "# TODO: implement the training loop, validating after every epoch, and make the requested plots\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: redo (c) with a ResNet\n",
    "# Hint: Training is probably quicker on google colab (https://colab.research.google.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
